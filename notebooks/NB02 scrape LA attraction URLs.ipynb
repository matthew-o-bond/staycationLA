{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collects URL for each attraction on city/region tripadvisor page\n",
    "#modified from https://github.com/giuseppegambino/Scraping-TripAdvisor-with-Python-2019/blob/master/tripadvisorSelenium.ipynb\n",
    "# and https://stackoverflow.com/questions/53965295/selenium-beautifulsoup-python-loop-through-multiple-pages/53965437"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd \n",
    "import glob\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check if the button is on the page, to avoid miss-click problem\n",
    "def check_exists_by_xpath(xpath):\n",
    "    try:\n",
    "        driver.find_element_by_xpath(xpath)\n",
    "    except NoSuchElementException:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count number of webpages per LA county location in tripadvisor\n",
    "def countpages(driver):\n",
    "    \n",
    "        #click \"see more\" if on page\n",
    "    if (check_exists_by_xpath(\"//div[contains(@class, '_3EzqC0V4') and contains(text(), 'more')]\")):\n",
    "        # to expand the review \n",
    "        driver.find_element_by_xpath(\"//div[@class='_3EzqC0V4']\").click()\n",
    "        time.sleep(4)\n",
    "    elif (check_exists_by_xpath(\"//span[@class='ui_icon single-chevron-down single-chevron-down']\")):\n",
    "        # to expand the review \n",
    "        driver.find_element_by_xpath(\"//span[@class='ui_icon single-chevron-down single-chevron-down']\").click()\n",
    "        time.sleep(4)   \n",
    "    elif (check_exists_by_xpath(\"//div[contains(@class, 'attractions-attraction-overview-main-TopPOIs__see_more--2Vsb-') and contains(text(), 'more']\")):\n",
    "        # to expand the review \n",
    "        driver.find_element_by_xpath(\"//div[@class='attractions-attraction-overview-main-TopPOIs__see_more--2Vsb-']\").click()\n",
    "        time.sleep(4)\n",
    "         \n",
    "    html_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    #pull out list of page numbers\n",
    "    if (check_exists_by_xpath('//a[contains(@class, \"_2Z-Porgu\")]')):\n",
    "        post_content = html_soup.find_all('a', class_ = '_2Z-Porgu')\n",
    "        #find last element of text in list of page numbers\n",
    "        last_div = None\n",
    "        for last_div in post_content:pass\n",
    "        if last_div:\n",
    "            content = last_div.getText()\n",
    "        else:\n",
    "            content=0\n",
    "    elif (check_exists_by_xpath('//div[contains(@class, \"attractions-attraction-overview-main-Pagination__link--2m5mV attractions-attraction-overview-main-Pagination__selected--2updu attractions-attraction-overview-main-Pagination__cx_brand_refresh_phase2--3XKui\")]')):\n",
    "        post_content = html_soup.find_all('div', class_ = \"attractions-attraction-overview-main-Pagination__link--2m5mV attractions-attraction-overview-main-Pagination__cx_brand_refresh_phase2--3XKui\")   \n",
    "        #find last element of text in list of page numbers\n",
    "        last_div = None\n",
    "        for last_div in post_content:pass\n",
    "        if last_div:\n",
    "            content = last_div.getText()\n",
    "        else:\n",
    "            content=0\n",
    "    else:\n",
    "        content=1\n",
    "\n",
    "    maxPageCount = int(content)\n",
    "    return(maxPageCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get URL for each attraction on the tripadvisor page\n",
    "#multiple if/else statements account for source code differences on different location pages\n",
    "def scrape(driver):\n",
    "    pageCounter = 0\n",
    "    while (pageCounter < maxPageCount):\n",
    "\n",
    "        #click \"see more\" if on page\n",
    "        if (check_exists_by_xpath(\"//div[contains(@class, '_3EzqC0V4') and contains(text(), 'more')]\")):\n",
    "            # to expand the review \n",
    "            driver.find_element_by_xpath(\"//div[@class='_3EzqC0V4']\").click()\n",
    "            time.sleep(4)\n",
    "        elif (check_exists_by_xpath(\"//div[contains(@class, 'attractions-attraction-overview-main-TopPOIs__see_more--2Vsb-') and contains(text(), 'more']\")):\n",
    "            # to expand the review \n",
    "            driver.find_element_by_xpath(\"//div[@class='attractions-attraction-overview-main-TopPOIs__see_more--2Vsb-']\").click()\n",
    "            time.sleep(4)\n",
    "        elif (check_exists_by_xpath(\"//span[@class='ui_icon single-chevron-down single-chevron-down']\")):\n",
    "            # to expand the review \n",
    "            driver.find_element_by_xpath(\"//span[@class='ui_icon single-chevron-down single-chevron-down']\").click()\n",
    "            time.sleep(4)    \n",
    "\n",
    "        #count number of attractions on page\n",
    "        if (check_exists_by_xpath(\"//div[contains(@class, '_2j03JUe9') and contains(@class, ' _2')]\")): \n",
    "            attraction_container = driver.find_elements_by_xpath(\"//div[contains(@class, '_2j03JUe9') and contains(@class, ' _20Q')]\")\n",
    "            num_page_items = len(attraction_container)    \n",
    "        elif (check_exists_by_xpath(\"//li[contains(@class, 'attractions-attraction-overview-pois-PoiCard__item--3UzYK')]\")): \n",
    "            attraction_container = driver.find_elements_by_xpath(\"//li[contains(@class, 'attractions-attraction-overview-pois-PoiCard__item--3UzYK')]\")\n",
    "            num_page_items = len(attraction_container)\n",
    "        elif (check_exists_by_xpath(\"//div[contains(@class, 'attractions-attraction-filtered-main-index__listItem--3trCl')]\")): \n",
    "            attraction_container = driver.find_elements_by_xpath(\"//div[contains(@class, 'attractions-attraction-filtered-main-index__listItem--3trCl')]\")\n",
    "            num_page_items = len(attraction_container)\n",
    "\n",
    "\n",
    "        #save url for each attraction in a csv file   \n",
    "        for j in range(num_page_items):\n",
    "            if (check_exists_by_xpath(\"//a[contains(@class, '_255i5rcQ')]\")): \n",
    "                link = attraction_container[j].find_element_by_xpath(\".//a[@class='_255i5rcQ']\").get_attribute(\"href\") #'_3s_5RiH-'\n",
    "                csvWriter.writerow([link])            \n",
    "            elif (check_exists_by_xpath(\"//a[@class='_1QKQOve4']\")):       \n",
    "                link = attraction_container[j].find_element_by_xpath(\".//a[@class='_1QKQOve4']\").get_attribute(\"href\") \n",
    "                csvWriter.writerow([link])         \n",
    "            elif (check_exists_by_xpath(\"//a[contains(@rel, 'noopener noreferrer')]\")):       \n",
    "                link = attraction_container[j].find_element_by_xpath(\".//a[@rel='noopener noreferrer']\").get_attribute(\"href\") \n",
    "                csvWriter.writerow([link]) \n",
    "\n",
    "\n",
    "\n",
    "        # to change the page\n",
    "        next_page_check = (pageCounter +2)\n",
    "        if (check_exists_by_xpath('//a[contains(@class, \"_2Z-Porgu\") and contains(text(), next_page_check)]')):\n",
    "            next_button = driver.find_element_by_xpath('//a[contains(@class, \"_2Z-Porgu\") and contains(text(), next_page_check)]')\n",
    "            action = ActionChains(driver)\n",
    "            action.move_to_element(next_button)        \n",
    "            action.click().perform()\n",
    "            time.sleep(5)               \n",
    "            csvFile.flush() #makes sure all scraped content is in csv file \n",
    "            pageCounter +=1\n",
    "            print(pageCounter) \n",
    "        elif (check_exists_by_xpath('//a[@class=\"ui_button nav next primary \"]')):\n",
    "            next_button=driver.find_element_by_xpath(\"//a[@class='ui_button nav next primary ']\")\n",
    "            action = ActionChains(driver)\n",
    "            action.move_to_element(next_button)        \n",
    "            action.click().perform()          \n",
    "            time.sleep(5)               \n",
    "            csvFile.flush() #makes sure all scraped content is in csv file  \n",
    "            pageCounter +=1\n",
    "            print(pageCounter) \n",
    "        elif (check_exists_by_xpath('//div[contains(@class, \"link--2m5mV attractions-attraction-overview-main-Pagination__cx_brand_refresh_phase2--3XKui ui_button primary attractions-attraction-overview-main-Pagination__button--1up7M\")]')):\n",
    "            next_button = driver.find_element_by_xpath('//div[contains(@class, \"link--2m5mV attractions-attraction-overview-main-Pagination__cx_brand_refresh_phase2--3XKui ui_button primary attractions-attraction-overview-main-Pagination__button--1up7M\")]')\n",
    "            action = ActionChains(driver)\n",
    "            action.move_to_element(next_button)        \n",
    "            action.click().perform()\n",
    "            time.sleep(5)               \n",
    "            csvFile.flush() #makes sure all scraped content is in csv file \n",
    "            pageCounter +=1\n",
    "            print(pageCounter) \n",
    "        else:\n",
    "            #to get reviews on last page or if tripadvisor shows more pages than actually exist\n",
    "            csvFile.flush() #makes sure all scraped content is in csv file \n",
    "            pageCounter +=1\n",
    "            print(pageCounter)\n",
    "            print(\"end\")\n",
    "\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.dropna of               class             name  \\\n",
      "0              city     agoura_hills   \n",
      "1              city         alhambra   \n",
      "2              city          arcadia   \n",
      "3              city          artesia   \n",
      "4              city           avalon   \n",
      "..              ...              ...   \n",
      "222  unincorporated         westmont   \n",
      "223  unincorporated      willowbrook   \n",
      "224  unincorporated  wilsona_gardens   \n",
      "225  unincorporated    windsor_hills   \n",
      "226  unincorporated         wiseburn   \n",
      "\n",
      "                                               website  \n",
      "0    https://www.tripadvisor.com/Attractions-g29069...  \n",
      "1    https://www.tripadvisor.com/Attractions-g29078...  \n",
      "2    https://www.tripadvisor.com/Attractions-g29105...  \n",
      "3    https://www.tripadvisor.com/Attractions-g29112...  \n",
      "4    https://www.tripadvisor.com/Attractions-g29121...  \n",
      "..                                                 ...  \n",
      "222                                                NaN  \n",
      "223                                                NaN  \n",
      "224                                                NaN  \n",
      "225                                                NaN  \n",
      "226                                                NaN  \n",
      "\n",
      "[227 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Control delimiters, rows, column names with read_csv (see later) \n",
    "locales = pd.read_csv(\"../LA.csv\") \n",
    "# Preview the first 5 lines of the loaded data \n",
    "#locales.head()\n",
    "print(locales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              class                   name  \\\n",
      "0              city           agoura_hills   \n",
      "1              city               alhambra   \n",
      "2              city                arcadia   \n",
      "3              city                artesia   \n",
      "4              city                 avalon   \n",
      "..              ...                    ...   \n",
      "162  unincorporated         marina_del_rey   \n",
      "176  unincorporated            pearblossom   \n",
      "185  unincorporated        rowland_heights   \n",
      "189  unincorporated  santa_catalina_island   \n",
      "204  unincorporated                topanga   \n",
      "\n",
      "                                               website  \n",
      "0    https://www.tripadvisor.com/Attractions-g29069...  \n",
      "1    https://www.tripadvisor.com/Attractions-g29078...  \n",
      "2    https://www.tripadvisor.com/Attractions-g29105...  \n",
      "3    https://www.tripadvisor.com/Attractions-g29112...  \n",
      "4    https://www.tripadvisor.com/Attractions-g29121...  \n",
      "..                                                 ...  \n",
      "162  https://www.tripadvisor.com/Attractions-g32684...  \n",
      "176  https://www.tripadvisor.com/Attractions-g32866...  \n",
      "185  https://www.tripadvisor.com/Attractions-g32992...  \n",
      "189  https://www.tripadvisor.com/Attractions-g10287...  \n",
      "204  https://www.tripadvisor.com/Attractions-g10528...  \n",
      "\n",
      "[98 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#remove duplicates\n",
    "locales2=locales.dropna(axis=0, how='any')\n",
    "print(locales2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "2\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "2\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "2\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "2\n",
      "3\n",
      "end\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "1\n",
      "end\n",
      "1\n",
      "2\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "2\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "2\n",
      "end\n",
      "3\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "2\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "2\n",
      "3\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "2\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n",
      "1\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "for locale_name, URL in zip(locales2['name'], locales2['website']):\n",
    "       # open the file to save the review\n",
    "    csvFile = open(\"/home/matthew/anaconda3/envs/insight/staycationLA/data/raw/LA_\" + locale_name + \".csv\", 'a')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "        # import the webdriver, chrome driver is recommended\n",
    "    driver = webdriver.Chrome(\"/home/matthew/Downloads/chromedriver\")\n",
    "    # insert the tripadvisor's website of one attraction \n",
    "    driver.get(URL)\n",
    "    time.sleep(5)\n",
    "    maxPageCount=countpages(driver)\n",
    "    time.sleep(1)\n",
    "    scrape(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in future add clicker for if surveys pop up and block buttons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.chdir(\"/home/matthew/anaconda3/envs/insight/staycationLA/data/raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('LA_*.{}'.format(extension))]\n",
    "#print(all_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all files in the list\n",
    "li = []\n",
    "\n",
    "for filename in all_filenames:\n",
    "    df = pd.read_csv(filename, index_col=None, names=[\"URL\"])\n",
    "    li.append(df)\n",
    "\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "#export to csv\n",
    "frame.to_csv( \"../processed/LA_combined_URLs.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
